{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Using the StableBaselines3 library for reinforcement learning\n",
    "\n",
    "In this notebook we test an implementation of the proximal policy optimization (PPO)\n",
    "PPO is described in detail in https://arxiv.org/abs/1707.06347. It is a variant of Trust Region Policy Optimization (TRPO) described (in this paper )[https://arxiv.org/abs/1502.05477]. The PPO algorithm works in two phases. In one phase, a large number of rollouts are performed (in parallel). The rollouts are then aggregated on the driver and a surrogate optimization objective is defined based on those rollouts. We then use SGD to find the policy that maximizes that objective with a penalty term for diverging too much from the current policy.\n",
    "\n",
    "![ppo](https://raw.githubusercontent.com/ucbrise/risecamp/risecamp2018/ray/tutorial/rllib_exercises/ppo.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "LB3eLu4qgikf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing the required libraries and our OpenAI-Gym compatible environment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import io\n",
    "import warnings\n",
    "\n",
    "import gym\n",
    "from PIL import Image\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from src.utils import boxes_generator"
   ],
   "metadata": {
    "id": "kZ0M4QIzuqCV",
    "outputId": "eac9dda6-7b79-440a-be99-a71bf5839034",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def make_env(\n",
    "        container_size,\n",
    "        num_boxes,\n",
    "        num_visible_boxes=1,\n",
    "        seed=0,\n",
    "        render_mode=None,\n",
    "        random_boxes=False,\n",
    "        only_terminal_reward=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "\n",
    "    ----------\n",
    "    container_size: size of the container\n",
    "    num_boxes: number of boxes to be packed\n",
    "    num_visible_boxes: number of boxes visible to the agent\n",
    "    seed: seed for RNG\n",
    "    render_mode: render mode for the environment\n",
    "    random_boxes: whether to use random boxes or not\n",
    "    only_terminal_reward: whether to use only terminal reward or not\n",
    "    \"\"\"\n",
    "    env = gym.make(\n",
    "        \"PackingEnv-v0\",\n",
    "        container_size=container_size,\n",
    "        box_sizes=boxes_generator(container_size, num_boxes, seed),\n",
    "        num_visible_boxes=num_visible_boxes,\n",
    "        render_mode=render_mode,\n",
    "        random_boxes=random_boxes,\n",
    "        only_terminal_reward=only_terminal_reward,\n",
    "    )"
   ],
   "metadata": {
    "id": "Hmtegq1thfbj",
    "outputId": "5d01bd0e-a91e-4dfa-90d6-90e45911869c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we set up the environment for training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "container_size = [10, 10, 10]\n",
    "box_sizes2 = [[3, 3, 3], [3, 2, 3], [3, 4, 2], [3, 2, 4], [3, 2, 3]]\n",
    "\n",
    "orig_env = gym.make(\n",
    "    \"PackingEnv-v0\",\n",
    "    container_size=container_size,\n",
    "    box_sizes=box_sizes2,\n",
    "    num_visible_boxes=5,\n",
    "    render_mode=\"human\",\n",
    "    only_terminal_reward=True,\n",
    ")\n",
    "\n",
    "env = gym.make(\n",
    "    \"PackingEnv-v0\",\n",
    "    box_sizes=box_sizes2,\n",
    "    container_size=container_size,\n",
    "    num_visible_boxes=5,\n",
    "    render_mode=\"human\",\n",
    "    only_terminal_reward=True,\n",
    ")\n",
    "\n",
    "check_env(env, warn=True)"
   ],
   "metadata": {
    "id": "6G0GQanQiRqw",
    "outputId": "83de09c6-46ba-402e-9763-a3efee3c11d9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We train the agent with the default multi-input policy that uses an MLP."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = MaskablePPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "print(\"begin training\")\n",
    "model.learn(total_timesteps=10000)\n",
    "print(\"done training\")\n",
    "model.save(\"ppo_mask\")"
   ],
   "metadata": {
    "id": "KMCuiduujfKh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "begin training\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5        |\n",
      "|    ep_rew_mean     | 0.111    |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5          |\n",
      "|    ep_rew_mean          | 0.111      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 8          |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 494        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01827472 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.56      |\n",
      "|    explained_variance   | -33.2      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.098     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0657    |\n",
      "|    value_loss           | 0.00492    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.06        |\n",
      "|    ep_rew_mean          | 0.111       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 725         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052354664 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.46       |\n",
      "|    explained_variance   | -78.9       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.145      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0871     |\n",
      "|    value_loss           | 0.000403    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.04       |\n",
      "|    ep_rew_mean          | 0.111      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 8          |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 948        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03723038 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.26      |\n",
      "|    explained_variance   | -46.7      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.106     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0736    |\n",
      "|    value_loss           | 0.000225   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5          |\n",
      "|    ep_rew_mean          | 0.111      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 6          |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 1469       |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02292182 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.26      |\n",
      "|    explained_variance   | -30.6      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0949    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0733    |\n",
      "|    value_loss           | 0.000198   |\n",
      "----------------------------------------\n",
      "done training\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we roll out the trained agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "done packing\n"
     ]
    }
   ],
   "source": [
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "\n",
    "obs = orig_env.reset()\n",
    "done = False\n",
    "figs = []\n",
    "fig = orig_env.render(mode=\"human\")\n",
    "fig_png = fig.to_image(format=\"png\")\n",
    "buf = io.BytesIO(fig_png)\n",
    "img = Image.open(buf)\n",
    "figs.append(img)\n",
    "step = 1\n",
    "while not done:\n",
    "    print(step)\n",
    "    action_masks = get_action_masks(env)\n",
    "    action, _states = model.predict(obs, deterministic=False, action_masks=action_masks)\n",
    "    obs, rewards, done, info = orig_env.step(action)\n",
    "    fig = orig_env.render(mode=\"human\")\n",
    "    fig_png = fig.to_image(format=\"png\")\n",
    "    buf = io.BytesIO(fig_png)\n",
    "    img = Image.open(buf)\n",
    "    figs.append(img)\n",
    "    step += 1\n",
    "print(\"done packing\")\n",
    "orig_env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we save the rollout as a gif"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "figs[0].save('../gifs/train_5_boxes.gif', format='GIF',\n",
    "             append_images=figs[1:],\n",
    "             save_all=True,\n",
    "             duration=300, loop=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}