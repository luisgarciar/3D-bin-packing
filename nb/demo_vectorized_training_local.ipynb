{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Using the StableBaselines3 library for reinforcement learning\n",
    "\n",
    "In this notebook we test an implementation of the proximal policy optimization (PPO) method for the 3D bin packing environment.\n",
    "The PPO method is described in detail in [this paper](https://arxiv.org/abs/1707.06347). It is a variant of Trust Region Policy Optimization (TRPO) described in [this article](https://arxiv.org/abs/1502.05477). The PPO algorithm works in two phases. In one phase, a large number of rollouts are performed (in parallel). The rollouts are then aggregated on the driver and a surrogate optimization objective is defined based on those rollouts. We then use SGD to find the policy that maximizes that objective with a penalty term for diverging too much from the current policy.\n",
    "\n",
    "![ppo](https://raw.githubusercontent.com/ucbrise/risecamp/risecamp2018/ray/tutorial/rllib_exercises/ppo.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "LB3eLu4qgikf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now test the PPO algorithm with the 3D bin packing environment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "_rJlODKjgikm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import gym\n",
    "from numpy.typing import NDArray\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "from src.utils import boxes_generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def mask_fn(env: gym.Env) -> NDArray:\n",
    "    return env.get_action_mask\n",
    "\n",
    "def make_env(container_size, num_boxes, num_visible_boxes=1, seed=0, render_mode=\"rgb_array\",\n",
    "             random_boxes=False, only_terminal_reward=False):\n",
    "    \"\"\"Utility function for initializing bin packing env with action masking\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "\n",
    "    env = gym.make(\n",
    "        \"PackingEnv-v0\",\n",
    "        container_size=container_size,\n",
    "        box_sizes=boxes_generator(container_size, num_boxes, seed),\n",
    "        num_visible_boxes=num_visible_boxes,\n",
    "        render_mode=render_mode,\n",
    "        random_boxes=random_boxes,\n",
    "        only_terminal_reward=only_terminal_reward\n",
    "    )\n",
    "    env = ActionMasker(env, mask_fn)\n",
    "    return env\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PvOUNkKagikn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished initialization of vectorized environment\n",
      "beginning training\n",
      "Using cpu device\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 32\u001B[0m\n\u001B[1;32m     26\u001B[0m model \u001B[38;5;241m=\u001B[39m MaskablePPO(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMultiInputPolicy\u001B[39m\u001B[38;5;124m\"\u001B[39m, env, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.4\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, tensorboard_log\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../logs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m#checkpoint_callback = CheckpointCallback(\u001B[39;00m\n\u001B[1;32m     29\u001B[0m  \u001B[38;5;66;03m#   save_freq=10, save_path=\"../logs/\", name_prefix=\"rl_model\"\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m#)\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m \u001B[43mevaluate_policy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_eval_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreward_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m90\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m model\u001B[38;5;241m.\u001B[39mlearn(\u001B[38;5;241m50\u001B[39m, callback\u001B[38;5;241m=\u001B[39mcheckpoint_callback)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone training\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/3dbp/lib/python3.8/site-packages/sb3_contrib/common/maskable/evaluation.py:103\u001B[0m, in \u001B[0;36mevaluate_policy\u001B[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn, use_masking)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    102\u001B[0m     actions, states \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(observations, state\u001B[38;5;241m=\u001B[39mstates, deterministic\u001B[38;5;241m=\u001B[39mdeterministic)\n\u001B[0;32m--> 103\u001B[0m observations, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m current_rewards \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m rewards\n\u001B[1;32m    105\u001B[0m current_lengths \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/3dbp/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:171\u001B[0m, in \u001B[0;36mVecEnv.step\u001B[0;34m(self, actions)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;124;03mStep the environments with the given action\u001B[39;00m\n\u001B[1;32m    166\u001B[0m \n\u001B[1;32m    167\u001B[0m \u001B[38;5;124;03m:param actions: the action\u001B[39;00m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;124;03m:return: observation, reward, done, information\u001B[39;00m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_async(actions)\n\u001B[0;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/3dbp/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:56\u001B[0m, in \u001B[0;36mDummyVecEnv.step_wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvStepReturn:\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;66;03m# Avoid circular imports\u001B[39;00m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[0;32m---> 56\u001B[0m         obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_rews[env_idx], terminated, truncated, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_infos[env_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;66;03m# convert to SB3 VecEnv api\u001B[39;00m\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_dones[env_idx] \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "File \u001B[0;32m/opt/anaconda3/envs/3dbp/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:94\u001B[0m, in \u001B[0;36mMonitor.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneeds_reset:\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTried to step environment that needs reset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 94\u001B[0m observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards\u001B[38;5;241m.\u001B[39mappend(reward)\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated:\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# Environment initialization\n",
    "container_size = [5, 5, 5]\n",
    "num_boxes = 10\n",
    "num_visible_boxes = 10\n",
    "num_env = 2\n",
    "env_kwargs = dict(\n",
    "    container_size=container_size,\n",
    "    num_boxes=num_boxes,\n",
    "    num_visible_boxes=num_visible_boxes,\n",
    "    render_mode=\"rgb_array\",\n",
    "    seed=42,\n",
    "    random_boxes=True,\n",
    "    only_terminal_reward=False)\n",
    "env = make_vec_env(make_env, n_envs=num_env, env_kwargs=env_kwargs)\n",
    "print(\"finished initialization of vectorized environment\")\n",
    "print(\"beginning training\")\n",
    "\n",
    "# MaskablePPO initialization\n",
    "model = MaskablePPO(\"MultiInputPolicy\", env, gamma=0.4, verbose=1, tensorboard_log=\"../logs\")\n",
    "\n",
    "#checkpoint_callback = CheckpointCallback(\n",
    " #   save_freq=10, save_path=\"../logs/\", name_prefix=\"rl_model\"\n",
    "#)\n",
    "\n",
    "evaluate_policy(model, env, n_eval_episodes=20, reward_threshold=90, warn=False)\n",
    "model.learn(50, callback=checkpoint_callback)\n",
    "print(\"done training\")\n",
    "model.save(\"../models/ppo_mask_cont555_boxes10_vis10_steps_50_numenv_2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MaskablePPO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mstable_baselines3\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvec_env\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvec_monitor\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VecMonitor\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mMaskablePPO\u001B[49m\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../models/ppo_mask_cont555_boxes10_vis10_steps_50_numenv_2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m num_env \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m      8\u001B[0m env_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m      9\u001B[0m     container_size\u001B[38;5;241m=\u001B[39mcontainer_size,\n\u001B[1;32m     10\u001B[0m     num_boxes\u001B[38;5;241m=\u001B[39mnum_boxes,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m     random_boxes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     15\u001B[0m     only_terminal_reward\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'MaskablePPO' is not defined"
     ]
    }
   ],
   "source": [
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
    "import os\n",
    "\n",
    "model = MaskablePPO.load(\"../models/ppo_mask_cont555_boxes10_vis10_steps_50_numenv_2\")\n",
    "\n",
    "num_env = 2\n",
    "env_kwargs = dict(\n",
    "    container_size=container_size,\n",
    "    num_boxes=num_boxes,\n",
    "    num_visible_boxes=num_visible_boxes,\n",
    "    render_mode=\"rgb_array\",\n",
    "    seed=42,\n",
    "    random_boxes=True,\n",
    "    only_terminal_reward=False)\n",
    "\n",
    "eval_env = make_vec_env(make_env, n_envs=num_env, env_kwargs=env_kwargs)\n",
    "log_dir = \"../eval/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "eval_env = VecMonitor(eval_env, log_dir)\n",
    "\n",
    "print(\"beginning evaluation\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')\n",
    "\n",
    "# obs = env.reset()\n",
    "# while True:\n",
    "#     # Retrieve current action mask\n",
    "#     action_masks = get_action_masks(env)\n",
    "#     action, _states = model.predict(obs, action_masks=action_masks)\n",
    "#     obs, rewards, dones, info = env.step(action)\n",
    "#     env.render()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from plotly_gif import GIF\n",
    "\n",
    "gif = GIF(gif_name=\"555_10box_10vis_200steps.gif\", gif_path=\"../gifs\")\n",
    "\n",
    "container_size = [5, 5, 5]\n",
    "num_boxes = 10\n",
    "num_visible_boxes = 10\n",
    "seed = 33\n",
    "env_kwargs = dict(\n",
    "    container_size=container_size,\n",
    "    num_boxes=num_boxes,\n",
    "    num_visible_boxes=num_visible_boxes,\n",
    "    render_mode=\"human\",\n",
    "    seed=seed,\n",
    "\n",
    ")\n",
    "\n",
    "eval_env = make_vec_env(make_env, n_envs=2, env_kwargs=env_kwargs)\n",
    "\n",
    "done = False\n",
    "obs = eval_env.reset()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/3D-bin-packing/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:179: UserWarning: Render not defined for <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x13d0c9430>\n",
      "  warnings.warn(f\"Render not defined for {self}\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'write_image'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m     obs, rewards, dones, info \u001B[38;5;241m=\u001B[39m eval_env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m      5\u001B[0m     fig \u001B[38;5;241m=\u001B[39m eval_env\u001B[38;5;241m.\u001B[39mrender()\n\u001B[0;32m----> 6\u001B[0m     \u001B[43mgif\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m gif\u001B[38;5;241m.\u001B[39mcreate_gif(length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5000\u001B[39m)\n\u001B[1;32m      9\u001B[0m fig \u001B[38;5;241m=\u001B[39m eval_env\u001B[38;5;241m.\u001B[39mcontainer\u001B[38;5;241m.\u001B[39mplot()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/3D-bin-packing/lib/python3.9/site-packages/plotly_gif/gif.py:114\u001B[0m, in \u001B[0;36mGIF.create_image\u001B[0;34m(self, fig, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuffer\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    113\u001B[0m     buffer \u001B[38;5;241m=\u001B[39m BytesIO()\n\u001B[0;32m--> 114\u001B[0m     \u001B[43mfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_image\u001B[49m(buffer, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpng\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    115\u001B[0m     buffer\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    116\u001B[0m     img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(buffer)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'write_image'"
     ]
    }
   ],
   "source": [
    "while not done:\n",
    "    # Retrieve current action mask\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = eval_env.step(action)\n",
    "    fig = eval_env.render()\n",
    "    gif.create_image(fig)\n",
    "\n",
    "gif.create_gif(length=5000)\n",
    "fig = eval_env.container.plot()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}